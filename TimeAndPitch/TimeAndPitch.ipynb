{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"right\"><i>COM418 - Computers and Music</i></div>\n",
    "<div align=\"right\"><a href=\"https://people.epfl.ch/paolo.prandoni\">Paolo Prandoni</a> and <a href=\"https://people.epfl.ch/lucie.perrotta\">Lucie Perrotta</a>, <a href=\"https://www.epfl.ch/labs/lcav/\">LCAV, EPFL</a></div>\n",
    "\n",
    "<p style=\"font-size: 30pt; font-weight: bold; color: #B51F1F;\">Pitch shifting and time stretching</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.signal as sp\n",
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 14, 4 \n",
    "plt.rcParams['image.cmap'] = 'tab10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def load_audio(filename):\n",
    "    x_sf, x = wavfile.read(filename)\n",
    "    x = (x - np.mean(x)) / 32767.0\n",
    "    return x, x_sf\n",
    "\n",
    "def multiplay(clips, sf, title=None):\n",
    "    outs = [widgets.Output() for c in clips]\n",
    "    for ix, item in enumerate(clips):\n",
    "        with outs[ix]:\n",
    "            print(title[ix] if title is not None else \"\")\n",
    "            display(Audio(item, rate=sf, normalize=True))\n",
    "    return widgets.HBox(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_spec(x, Fs, max_freq=None, do_fft=True):\n",
    "    C = int(len(x) / 2)  # positive frequencies only\n",
    "    if max_freq:\n",
    "        C = int(C * max_freq / float(Fs) * 2) \n",
    "    X = np.abs(np.fft.fft(x)[0:C]) if do_fft else x[0:C]\n",
    "    N = Fs * np.arange(0, C) / len(x);\n",
    "    plt.plot(N, X)\n",
    "    return N, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ss, ssf = load_audio('snd/speech.wav')\n",
    "ms, msf = load_audio('snd/music.wav')\n",
    "cs, csf = load_audio('snd/cymbal.wav')\n",
    "ks, ksf = load_audio('snd/clarinet.wav')\n",
    "ys, ysf = load_audio('snd/yesterday.wav')\n",
    "assert ssf == msf == csf == ksf == ysf, 'notebook requires sampling rates for all audio samples to be the same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The initial tradeoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Speed, pitch, timbre\n",
    "\n",
    " * the speed of an audio signal is related to the time interval between events, eg $ x_\\mathrm{start} = x(t_0), x_\\mathrm{stop} = x(t_1), \\Delta_t = t_1 - t_0 $\n",
    " * the sensation of _pitch_ is related to the (local) periodicity of a signal: if $ x(t) \\approx x(t + nP) $, pitch \"frequency\" is $F = 1/P$\n",
    " * the _timbre_ is related to the spectrum $ X(f) $\n",
    " \n",
    "Ideal scenario: being able to change pitch and speed independently without changing timbre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Time scaling\n",
    "\n",
    "Changing the time scale is a linear operation: $x'(t) = x(t / \\alpha)$\n",
    "\n",
    "Changing the time scale affects everything at once:\n",
    " * speed is scaled by $\\alpha$ as $\\Delta'_t = \\alpha \\Delta_t $: faster if $\\alpha < 1$, slower otherwise\n",
    " * period is changed as $P' = \\alpha P$ and pitch as $F' = F/\\alpha$: higher pitch if $\\alpha < 1$, lower otherwise\n",
    " * timbre is changed as $X'(f) = X(\\alpha f)$ so that if $\\alpha < 1$ frequencies are stretched (chipmunk) otherwise they are contracted (Darth Vader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analog time scaling\n",
    "\n",
    "<img width=\"400\" style=\"\" src=\"img/turntable.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Digital time scaling\n",
    "\n",
    "#### Method #1\n",
    "<img width=\"600\" style=\"float:right;\" src=\"img/multirate.png\">\n",
    "\n",
    " * $\\alpha = N/M$\n",
    " * upsample by $N$\n",
    " * downsample by $M$\n",
    " \n",
    "can get needlessly expensive for large values of $M, N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Method #2\n",
    "\n",
    "Fractional resampling: for each output index $m$:\n",
    "\n",
    " * compute the closest input index $n = \\lfloor \\alpha m \\rfloor$\n",
    " * linearly interpolate between $x[n]$ and $x[n+1]$ at $\\tau = \\alpha m - n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def resample(x, alpha):\n",
    "    # length of the output signal after resampling\n",
    "    n_out = int(np.floor(len(x) * alpha))\n",
    "    y = np.zeros(n_out)\n",
    "    for iy in range(0, n_out - 1):\n",
    "        t = iy / alpha \n",
    "        ix = int(t)\n",
    "        y[iy] = (1 - (t - ix)) * x[ix] + (t - ix) * x[ix + 1] \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "multiplay([ss, resample(ss, 0.6), resample(ss, 1.5)], ssf, title=['speech sample', 'sped up via resampling', 'slowed down via resampling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "multiplay([ms, resample(ms, 0.6), resample(ms, 1.5)], msf, title=['music sample', 'sped up via resampling', 'slowed down via resampling'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Issues with time scaling:\n",
    "\n",
    " * if used to change duration: pitch and timbre also affected\n",
    " * if used to change pitch: duration and timbre also affected\n",
    " * cannot be implemented in real time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequency shifting\n",
    "\n",
    "### Naive upmodulation\n",
    "Remember the modulation theorem: $\\mathrm{FT}\\{x(t)\\cos(2\\pi f_0 t)\\} = [X(f - f_0) + X(f + f_0)]/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitchshift_mod(x, f, sf):\n",
    "    return x * np.cos(2 * np.pi * f / sf * np.arange(0, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplay([ss, pitchshift_mod(ss, 400, ssf)], ssf, title=['speech sample', 'shifted up (simple modulation)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img width=\"250\" style=\"float: left; margin: 0px 120px 0 0;\" src=\"img/voice_changer.jpg\">\n",
    "\n",
    "Good enough for a prank call but:\n",
    "\n",
    " * audio is baseband, so we can only go \"up\"\n",
    " * shift is smaller than effective bandwidth: aliasing\n",
    " * inaudible low frequencies are brought into hearing range (warbling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Better spectral shifting\n",
    "\n",
    "<img width=\"600\" style=\"float: right;\" src=\"img/ssb.png\">\n",
    "\n",
    "Take a page out of SSB (single-sideband modulation)\n",
    "\n",
    " * bandpass filter the audio to eliminate low frequencies and limit bandwidth\n",
    " * if shifting down, eliminate low frequencies below modulation frequency\n",
    " * compute analytic signal using a FIR approximation to the Hilbert filter\n",
    " * shift using complex exponential\n",
    " * take real part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pitchshift_ssb(x, f, sf, L=40):\n",
    "    # bandpass the voice signal\n",
    "    f0 = 100.0 if f > 0 else 100.0 - f\n",
    "    x = sp.lfilter(*sp.butter(6, [f0, min(10000, 0.9 * sf/2)], fs=sf, btype='band'), x)\n",
    "    # compute analytic signal\n",
    "    h = sp.remez(2 * L + 1, [0.02, 0.48], [-1], type='hilbert')\n",
    "    xa = 1j * sp.lfilter(h, 1, x)[L:] + x[:-L]\n",
    "    # shift and take real part\n",
    "    return np.real(xa * np.exp(1j * 2 * np.pi * f / sf * np.arange(0, len(xa))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "multiplay([ss, pitchshift_ssb(ss, 400, ssf), pitchshift_ssb(ss, -400, ssf)], ssf, title=['speech sample', 'shifted up (SSB)', 'shifted down (SSB)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplay([ss, pitchshift_mod(ss, 400, ssf), pitchshift_ssb(ss, 400, ssf)], ssf, title=['speech sample', 'simple up-modulation', 'ssb up-modulation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture spectra\n",
    "for n, x in enumerate([ss, pitchshift_mod(ss, 400, ssf), pitchshift_ssb(ss, 400, ssf)]):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plot_spec(x, ssf, 2000)\n",
    "    if n > 0:\n",
    "        plt.axvline(400, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " * spectrum of modulated signal shows lots of spurious content in low frequency region\n",
    " * modulated signal has aliasing (see the two peaks at $400 \\pm 150$Hz\n",
    " * SSB-modulated signal is much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### _The_ issue with spectral shifting\n",
    "\n",
    "Good things so far: \n",
    " * can be implemented in real time\n",
    " * preserves the speed of the original audio\n",
    " \n",
    "But:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplay([ms, pitchshift_ssb(ms, 400, ssf), pitchshift_ssb(ms, -400, ssf)], ssf, title=['music sample', 'shifted up (SSB)', 'shifted down (SSB)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pitch perception requires a _harmonic_ spectral structure, with spectral lines at multiples of a fundamental frequency:\n",
    "\n",
    "$$\n",
    "    f_n = nf_1, n = 1, 2, 3, \\ldots \\Rightarrow \\frac{f_1}{f_n} = \\frac{1}{n}\n",
    "$$\n",
    "\n",
    "Spectral shifting breaks the harmonic structure:\n",
    "\n",
    "$$\n",
    "    f'_n = \\Delta f + nf_1 \\Rightarrow \\frac{f'_1}{f'_n} = \\frac{\\Delta f + f_1}{\\Delta f + nf_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Granular Synthesis\n",
    "<img width=\"500\" style=\"float: right;\" src=\"img/gsplot.jpg\">\n",
    "\n",
    "In [Granular Synthesis](https://en.wikipedia.org/wiki/Granular_synthesis) complex waveforms are be built by stitching together very short sound snippets called \"grains\". \n",
    " \n",
    " * used as a compositional tool to generate complex timbres at arbitrary pitches\n",
    " * each grain must be \"pitched\"\n",
    " * works well for non-pitched sounds too\n",
    " * lots of sophisticated variations exist to maximize output quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Helper function to convert milliseconds to samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms2n(ms, sf):\n",
    "    return int(float(sf) * float(ms) / 1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time stretching via granular synthesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Proof of concept: grain repetition\n",
    "\n",
    " * split signal into small grain\n",
    " * repeat each grain two or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_poc(x, grain_size, M=2):\n",
    "    y = np.zeros(M * (len(x) + 1))\n",
    "    for n in range(0, len(x) - grain_size, grain_size):\n",
    "        y[M * n : M * (n + grain_size)] = np.tile(x[n : n + grain_size], M)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grain_size = ms2n(30, ssf)\n",
    "multiplay([gs_poc(ss, grain_size), gs_poc(ms, grain_size)], ssf, title=['slowed down speech', 'slowed down music'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, although the simple tiling of the grains creates discontinuities at the boundaries that result in _blocking artefacts_ (the clicking noise). We need to fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Crossfading grains\n",
    "\n",
    "First ingredient to mitigate artefacts: overlap and *crossfade* the grains via a a *tapering* window. \n",
    "\n",
    " * windows should fade to zero at both ends\n",
    " * grains should be overlapped so that the sum of windows is one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following function returns a simple tapering window with linear crossfading at the edges\n",
    " \n",
    " * the overlap parameter $0 \\le a \\le 1$ determines the *total* amount of taper (left and right)\n",
    " * the function also returns a *stride* value $S$ in samples, i.e. the amount of shift for correct overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tapering_window(N, overlap):\n",
    "    R = int(N * overlap / 2)\n",
    "    r = np.arange(0, R) / float(R)\n",
    "    win = np.r_[r, np.ones(N - 2*R), r[::-1]]\n",
    "    stride = N - R - 1 if R > 0 else N\n",
    "    return win, stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def test_overlap(win, stride, size=None, N = 3):\n",
    "    size = size if size else len(win)\n",
    "    y = np.zeros((N - 1) * stride + size)\n",
    "    for n in range(0, N):\n",
    "        plt.plot(np.arange(n * stride, n * stride + size), win, 'C0')\n",
    "        y[n*stride:n*stride+size] += win\n",
    "    plt.plot(y, 'C2', linewidth=8, alpha=0.3)\n",
    "    plt.gca().set_ylim([0, 1.1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_overlap(*tapering_window(100, 0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic pitch-preserving time stretching\n",
    "\n",
    "<img width=\"800\" style=\"float: right;\" src=\"img/ola.png\">\n",
    "\n",
    "For a stretch factor $\\alpha$: \n",
    "\n",
    " * choose grain size (around 40 ms for speech, 100 ms for music) \n",
    " * choose overlap (from 40% to 100%) \n",
    " * compute the output stride $S$ (in samples)\n",
    " * iterate:\n",
    "     * get an input grain at sample $k \\lfloor S / \\alpha \\rfloor$  \n",
    "     * blend the grain in the output signal at sample $kS$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def timescale_gs(x, alpha, grain_size, overlap=0.4):\n",
    "    win, stride = tapering_window(grain_size, overlap)\n",
    "    in_hop, out_hop = int(stride / alpha), stride\n",
    "    y = np.zeros(int(alpha * len(x)))\n",
    "    ix, iy = 0, 0\n",
    "    while ix < len(x) - grain_size and iy < len(y) - grain_size:\n",
    "        y[iy:iy+grain_size] += x[ix:ix+grain_size] * win\n",
    "        iy += out_hop\n",
    "        ix += in_hop\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grain_size = ms2n(40, ssf)\n",
    "multiplay([ss, timescale_gs(ss, 0.8, grain_size), timescale_gs(ss, 1.5, grain_size)], ssf, title=['speech sample', 'sped up (GS)', 'slowed down (GS)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_size = ms2n(100, msf)\n",
    "multiplay([ms, timescale_gs(ms, 0.8, grain_size), timescale_gs(ms, 1.5, grain_size)], ssf, title=['music sample', 'sped up (GS)', 'slowed down (GS)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remarks so far:\n",
    "  * time stretching works best to speed up speech\n",
    "  * slowed down speech still has clicking artefacts\n",
    "  * music is OK but the lower pitches have significant detuning\n",
    "  \n",
    "There seems to be a problem with low frequencies and still significant artefacts. We'll talk more about those later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pitch shifting via granular synthesis\n",
    "\n",
    "Idea:\n",
    " * use GS to stretch time by a factor $\\alpha$ without affecting pitch\n",
    " * use time scaling (i.e. resampling) by a factor $1/\\alpha$ to bring original time back and change pitch as a side effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic implementation\n",
    "\n",
    "In two passes, non real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitchshift_gs(x, alpha, grain_size, overlap=0.4):\n",
    "    return(resample(timescale_gs(x, alpha, grain_size, overlap), 1 / alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semitone = 2 ** (1.0 / 12)\n",
    "alpha, grain_size = semitone ** 2, ms2n(100, ssf)\n",
    "multiplay([ms, pitchshift_gs(ms, alpha, grain_size), pitchshift_gs(ms, 1 / alpha, grain_size)], ssf, title=['music sample', 'up two semitones (GS)', 'down two semitones (GS)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Real-time implementation\n",
    "\n",
    "Time stretching and resampling can be combined by synthesizing the output via *resampled grains*. \n",
    "\n",
    "Observaton: for a simple resampler, the (non-integer) input time index for an output sample index $n$ is $t = n/\\alpha$. Graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture ramps\n",
    "n = np.arange(0, 100)\n",
    "for ix, alpha in enumerate([1, 1.6, 0.6]):\n",
    "    plt.subplot(1, 3, ix+1)\n",
    "    plt.plot(resample(n, alpha)[:-1])\n",
    "    plt.gca().set_xlim([0, 100])\n",
    "    plt.xlabel('output index')    \n",
    "    plt.ylabel('input index')\n",
    "    plt.title(rf'$\\alpha$ = {alpha}')\n",
    "plt.subplots_adjust(wspace=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Idea: use granular synthesis but directly resample each input grain before using it in the output:\n",
    "\n",
    " * for a nomimal output grain size we will need to use a larger or smaller input chunk\n",
    " * the process goes through the input signal in a zig-zag patterns\n",
    " * since we are not changing the time scale, the input and output indices are aligned at the beginning of each grain\n",
    "\n",
    "(note that we use no overlap here for clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_map(n, alpha, grain_size):\n",
    "    # computes the fractional index inside an input grain for a given output index\n",
    "    # start sample for input grain\n",
    "    t = np.floor(n / grain_size) * grain_size\n",
    "    # fractional index in input grain\n",
    "    t += (n - t) * alpha\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture ramps\n",
    "n = np.arange(0, 100)\n",
    "for ix, alpha in enumerate([1, 1.6, 0.6]):\n",
    "    plt.subplot(1, 3, ix+1)\n",
    "    plt.plot(gs_map(n, alpha, 15))\n",
    "    plt.gca().set_xlim([0, 100])\n",
    "    plt.gca().set_ylim([0, 100])\n",
    "    plt.title(rf'$\\alpha$ = {alpha}')\n",
    "    plt.xlabel('output index')    \n",
    "    plt.ylabel('input index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ramps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "More in detail, in granular pitch shifting we generate output grains by performing fractional resampling on fixed-length input _chunks_; the start times for an input chunk and the corresponding output grain are synchronized but the number of input samples needed to produce the output grain will be larger or smaller than the size of the output grain according to whether we're raising or lowering the pitch. For instance, if the grain size is 100 samples and we are increasing the output frequency by 20%, we will need to use input chunks that are at least 120 samples long. To preserve the synchronicity of input and output, at the end of a grain generation process we will need to jump back in the input data by 20 samples. This way, the maximum buffer delay to produce a grain will be at most than $\\alpha G$ samples, where $G$ is the size of the grain and $\\alpha$ is the resampling factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a real-time granular pitch shifter using overlapping grains with a tapering window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitchshift_gs_rt(x, alpha, grain_size, overlap=0.4):\n",
    "    win, stride = tapering_window(grain_size, overlap)\n",
    "    # resampling needs these many input samples to produce an output grain of the chosen size\n",
    "    chunk_size = int(np.floor(grain_size + 1) * alpha)\n",
    "    y = np.zeros(len(x))\n",
    "    # input chunks and output grains are always aligned in pitch shifting (in_hop = out_hop = stride)\n",
    "    for n in range(0, len(x) - max(chunk_size, grain_size), stride):\n",
    "        y[n:n+grain_size] += resample(x[n:n+chunk_size], 1 / alpha) * win\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grain_size = ms2n(100, ssf)\n",
    "multiplay([ms, pitchshift_gs_rt(ms, semitone ** 2, grain_size), pitchshift_gs_rt(ms, semitone ** (-2), grain_size)], ssf, title=['music sample', 'up two semitones (GS-RT)', 'down two semitones (GS-RT)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Microcontroller implementation\n",
    "<img width=\"300\" style=\"float: right;\" src=\"img/microcontroller.gif\">\n",
    "\n",
    "[This gitbook](https://hwlab.learndsp.org/) shows how to implement a granular synthesis pitch shifter on a microcontroller:\n",
    " * very low memory \n",
    " * all signal processing must be done in fixed point for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Before DSP...\n",
    "\n",
    "<img width=\"600\" style=\"\" src=\"img/pitchshift.jpg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Although we have just described a purely digital version of grain-based pitch shifting, it is interesting to remark that, before digital audio was a reality, the only true pitch-shifting devices available to the music industry were extremely complex (and costly) mechanical devices that implemented, in analog, the same principle behind granular synthesis. \n",
    "\n",
    "Above is the block diagram of such a contraption: the original sound is recorded on the main tape spool, which is run at a speed that can vary with respect to the nominal recording speed to raise or lower the pitch. To compensate for these changes in speed the tape head is actually a rotating disk containing four individual coils; at any given time, at least two neighboring coils are picking up the signal from the tape, with an automatic fade-in and fade-out as they approach and leave the tape. The head disk rotates at a speed that compensates for the change in speed of the main tape, therefore keeping the timebase constant. The coils on the head disk picking up the signal are in fact producing overlapping \"grains\" that are mixed together in the output signal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# The phase vocoder\n",
    "\n",
    "Simple granular synthesis has still a lot of quality issues. Let's explore (and try to solve) the key problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The problem with \"simple\" granular synthesis\n",
    "\n",
    "Granular synthesis seems to work well on non pitched sounds but not so well on sustained periodic sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_size = ms2n(15, ssf)\n",
    "multiplay([cs, timescale_gs(cs, 1.5, grain_size)], ssf, title=['cymbal', 'stretched'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_size = ms2n(100, ssf)\n",
    "multiplay([ks, timescale_gs(ks, 1.5, grain_size)], ssf, title=['clarinet', 'stretched'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It will be easier to investigate the problem if we use a simple sinusoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sin = np.sin(2 * np.pi * (110 / ssf) * np.arange(0, ssf))\n",
    "plt.plot(test_sin[:2000]);\n",
    "Audio(test_sin, rate=ssf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_grain_size = 500\n",
    "test_sin_scaled = timescale_gs(test_sin, 2, test_grain_size)\n",
    "plt.plot(test_sin_scaled[1000:3000])\n",
    "Audio(test_sin_scaled, rate=ssf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The reason for the strange waveform becomes evident if we set the gain overlap to zero: phase jumps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(timescale_gs(test_sin, 2, test_grain_size, overlap=0)[0:2000])\n",
    "for n in range(test_grain_size, 2000, test_grain_size):\n",
    "    plt.axvline(n, color='red', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a single sinusoid the solution is easy:\n",
    " * compute the instantaneous final phase of the $n$-th grain $\\varphi_1(n)$ at the point of overlap\n",
    " * set the initial phase $\\varphi_0(n+1)$ of the next sinusoidal grain equal to $\\varphi_1(n)$\n",
    " \n",
    "But music is more than a simple sinusoid..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Phase adjustment in time stretching\n",
    "\n",
    "Several approaches for real-world signals; the most well-known is the phase vocoder algorithm:\n",
    " \n",
    " * compute the DFT of the input grain (i.e. decompose the grain into a set of harmonic sinusoids)\n",
    " * change its phase so that it aligns to the phase of the previous grain at the point of overlap\n",
    " * rebuild the grain via an inverse DFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Consider two segments of an audio signal and their superposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, A, B = 2048, 60000, 61500\n",
    "x1, x2 = ms[A:A+L], ms[B:B+L]\n",
    "plt.plot(x1)\n",
    "plt.plot(x2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that they are relatively out of phase and that they will not blend well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider now the same segments, but the phase of the second one has been set to that of the first: notice how the maxima and minima tend to align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x1)\n",
    "plt.plot(np.real(np.fft.ifft(np.abs(np.fft.fft(x2)) * 1j * np.angle(np.fft.fft(x1)))));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Phase propagation\n",
    "\n",
    "Phase continuity is achieved via a time-varying a _global_ phase vector $\\boldsymbol{\\varphi}_n$ like so:\n",
    " * the phase of the current input grain $\\boldsymbol{\\theta}_0$ is computed\n",
    " * the phase $\\boldsymbol{\\theta}_S$ of a grain-sized input chunk shifted by $S$ samples is computed (i.e. the expected phase at the point of overlap)\n",
    " * the current grain is resynthesized using the global phase vector\n",
    " * the global phase vector is updates as  $\\boldsymbol{\\varphi}_{n+1} = \\boldsymbol{\\varphi}_n + (\\boldsymbol{\\theta}_S - \\boldsymbol{\\theta}_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Hanning window\n",
    "\n",
    "Phase estimation is numerically delicate. To minimize errors a Hanning window with 100% overlap is used both in the DFT computations and to blend grains together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hanning_window(size):\n",
    "    # make sure size is odd\n",
    "    stride = size // 2\n",
    "    size = 2 * stride + 1\n",
    "    win = np.hanning(size)\n",
    "    return win, stride, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_overlap(*hanning_window(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The final algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timescale_gs_pv(x, alpha, grain_size):\n",
    "    # we will use an odd-length Hanning window with 100% overlap\n",
    "    win, stride, grain_size = hanning_window(grain_size)\n",
    "    in_hop, out_hop = int(stride / alpha), stride\n",
    "    # initialize output phase with phase of first grain\n",
    "    phase = np.angle(np.fft.fft(x[0:grain_size]))\n",
    "    y, ix, iy = np.zeros(int(alpha * len(x))), 0, 0    \n",
    "    while ix < len(x) - 2 * grain_size and iy < len(y) - grain_size:\n",
    "        # FFT of current grain\n",
    "        grain_fft = np.fft.fft(win * x[ix:ix+grain_size])\n",
    "        # phase of the grain at the point of intersection with next grain in the output \n",
    "        end_phase = np.angle(np.fft.fft(win * x[ix+out_hop:ix+out_hop+grain_size]))\n",
    "        phase_diff = end_phase - np.angle(grain_fft)\n",
    "        # compute rephased grain and add with overlap to output\n",
    "        grain = np.real(np.fft.ifft(np.abs(grain_fft) * np.exp(1j * phase)))\n",
    "        y[iy:iy+grain_size] += grain * win\n",
    "        iy += out_hop\n",
    "        ix += in_hop\n",
    "        # update output phase for next grain and reduce modulo 2pi\n",
    "        phase = phase + phase_diff  \n",
    "        phase = phase - 2 * np.pi * np.round(phase / (2 * np.pi))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now test with the sinusoidal signal again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sin_scaled_pv = timescale_gs_pv(test_sin, 2, test_grain_size)\n",
    "plt.plot(test_sin_scaled_pv[1000:3000])\n",
    "multiplay([test_sin, test_sin_scaled, test_sin_scaled_pv], ssf, title=['test sinusoid', 'stretched with GS', 'stretched with GS-PV'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Speeding up music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, grain_size = 0.8, ms2n(100, ssf)\n",
    "multiplay([ms, timescale_gs(ms, alpha, grain_size), timescale_gs_pv(ms, alpha, grain_size)], ssf, title=['music sample', 'sped up with GS', 'sped up with GS-PV'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Slowing down music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, grain_size = 2, ms2n(100, ssf)\n",
    "multiplay([ms, timescale_gs(ms, alpha, grain_size), timescale_gs_pv(ms, alpha, grain_size)], ssf, title=['music sample', 'slowed down with GS', 'slowed down with GS-PV'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Speeding up speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, grain_size = 0.8, ms2n(40, ssf)\n",
    "multiplay([ss, timescale_gs(ss, alpha, grain_size), timescale_gs_pv(ss, alpha, grain_size)], ssf, title=['speech sample', 'sped up with GS', 'sped up with GS-PV'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Slowing down speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, grain_size = 2, ms2n(60, ssf)\n",
    "multiplay([ss, timescale_gs(ss, alpha, grain_size), timescale_gs_pv(ss, alpha, grain_size)], ssf, title=['speech sample', 'slowed down with GS', 'slowed down with GS-PV'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pitch shifting with the phase vocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic implementation (two passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitchshift_gs_pv_rt(x, alpha, grain_size):\n",
    "    return(resample(timescale_gs_pv(x, alpha, grain_size), 1 / alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Real-time implementation\n",
    "\n",
    "Differences with the non-phase compensated implementation:\n",
    " * need to rescale two consecutive input chunks\n",
    " * need to explicitly compute two DFT's per output grain to compute the rescaled signal phase offset\n",
    " * initialize the global phase at zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def pitchshift_gs_pv_rt(x, alpha, grain_size):\n",
    "    win, stride, grain_size = hanning_window(grain_size)\n",
    "    # resampling needs these many input samples to produce an output grain of the chosen size\n",
    "    chunk_size = int(np.floor(grain_size + 1) * alpha)\n",
    "    phase = np.zeros(grain_size)\n",
    "    y = np.zeros(len(x))\n",
    "    # input chunks and output grains are always aligned in pitch shifting (in_hop = out_hop = stride)\n",
    "    for n in range(0, len(x) - 2 * max(chunk_size, grain_size), stride):\n",
    "        # resample two contiguous chunks to compute the phase difference\n",
    "        resampled_chunk = resample(x[n:n+chunk_size+chunk_size], 1 / alpha)\n",
    "        grain_fft_curr = np.fft.fft(win * resampled_chunk[0:grain_size])\n",
    "        grain_fft_next = np.fft.fft(win * resampled_chunk[stride:stride+grain_size])\n",
    "        phase_diff = np.angle(grain_fft_next) - np.angle(grain_fft_curr)\n",
    "        # resynthesize current grain\n",
    "        grain = np.real(np.fft.ifft(np.abs(grain_fft_curr) * np.exp(1j * phase)))\n",
    "        y[n:n+grain_size] += grain * win\n",
    "        # update phase for next grain\n",
    "        phase = phase + phase_diff  \n",
    "        phase = phase - 2 * np.pi * np.round(phase / (2 * np.pi))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "compare the difference between normal and phase-compensate pitch shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, grain_size = semitone ** 2, ms2n(100, ssf)\n",
    "multiplay([ms, pitchshift_gs_rt(ms, alpha, grain_size), pitchshift_gs_pv_rt(ms, alpha, grain_size)], ssf, \n",
    "          title=['original (music)', 'up 2 semitones (GS)', 'up 2 semitones (GS-PV)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autotune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The phase vocoder, when applied to a singing voice, still produces slighty unnatural-sounding speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, grain_size = semitone ** 2, ms2n(100, ssf)\n",
    "multiplay([ys, pitchshift_gs_pv_rt(ys, alpha, grain_size), pitchshift_gs_pv_rt(ys, 1/alpha, grain_size)], ssf, title=['vocal sample', 'up two semitones', 'down two semitones'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The source-filter model for voice\n",
    "<img width=\"400\" style=\"float: right; margin-right: 30px;\" src=\"img/lpc.jpg\"> \n",
    "\n",
    "Sources:\n",
    " * vibration of vocal cords (voiced sounds)\n",
    " * noise-like air flow (unvoiced sounds)\n",
    " \n",
    "Filter:\n",
    " * vocal tract (larynx and mouth: time varying)\n",
    " * head and chest resonances (fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# load a short voiced speech segment and plot its spectrum and the corresponding envelope\n",
    "vs, vsf = load_audio('snd/voiced.wav');\n",
    "# envelope will be computed explicitly later, here we just plot it\n",
    "vs_env = np.fft.fft([1.0, -2.1793, 2.4140, -1.6790, 0.3626, 0.5618, -0.7047, \n",
    "                0.1956, 0.1872, -0.2878, 0.2354, -0.0577, -0.0815, 0.0946, \n",
    "                0.1242, -0.1360, 0.0677, -0.0622, -0.0306, 0.0430, -0.0169], len(vs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr --no-stdout voiced\n",
    "_, __ = plot_spec(vs, vsf)\n",
    "_, __ = plot_spec(np.abs(np.divide(1.0, vs_env)), vsf, do_fft=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The spectrum of a short voiced speech segment shows the harmonic nature of the sound and the overall envelope determined by the head and mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voiced.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Key observations:\n",
    "\n",
    " * the frequency response of the filter is independent of the excitation\n",
    " * to pitch-shift speech we must modify only the source\n",
    " * if we shift the envelope, the voice will sound unnatural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LPC analysis\n",
    "\n",
    "Premise: work on short voice segments so that both source and filter can be considered constant.\n",
    "\n",
    "Voice production model:\n",
    "\n",
    "$$\n",
    "    X(z) = A(z)E(z)\n",
    "$$\n",
    "\n",
    "We need to find _both_  $A(z)$ and $E(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The all-pole filter model\n",
    "\n",
    "Resonances in the vocal tract are adequately captured by an all-pole filter\n",
    "\n",
    "$$\n",
    "  A(z) = \\frac{1}{1 - \\sum_{k=1}^{p}a_kz^{-k}}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The AR estimation problem\n",
    "\n",
    "$$\n",
    "  x[n] = \\sum_{k=1}^{p}a_k x[n-k] + e[n]\n",
    "$$\n",
    "\n",
    "which becomes\n",
    "\n",
    "$$\n",
    "  e[n] = x[n] - \\sum_{k=1}^{p}a_k x[n-k]\n",
    "$$\n",
    "\n",
    "The optimal solution can be found by minimizing $E[e^2[n]]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $E[e^2[n]]$ is minimized, then $e[n]$ is orthogonal to $x[n]$ (see the [orhtogonality principle](https://en.wikipedia.org/wiki/Orthogonality_principle))\n",
    "\n",
    "Orthogonality means no shared information between signals: we have separated excitation and source!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The linear prediction coefficients\n",
    "\n",
    "The coefficients of the filter $A(z)$ can be found as\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        r_0 & r_1 & r_2 & \\ldots & r_{p-1} \\\\\n",
    "        r_1 & r_0 & r_1 & \\ldots & r_{p-2} \\\\        \n",
    "        & & & \\vdots \\\\\n",
    "        r_{p-1} & r_{p-2} & r_{p-3} & \\ldots & r_{0} \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        a_1 \\\\\n",
    "        a_2 \\\\        \n",
    "        \\vdots \\\\\n",
    "        a_{p}\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        r_1 \\\\\n",
    "        r_2 \\\\        \n",
    "        \\vdots \\\\\n",
    "        r_{p}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $r$ is the biased autocorrelation of the $N$-point input data:\n",
    "\n",
    "$$\n",
    "  r_m = (1/N)\\sum_{k = 0}^{N-m-1}x[k]x[k+m]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because of the Toeplitz structure of the autocorrelation matrix, the system of equations can be solved very efficiently using the Levinson-Durbin algorithm. Here is a direct implementation of the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ld(r, p):\n",
    "    # solve the toeplitz system using the Levinson-Durbin algorithm\n",
    "    g = r[1] / r[0]\n",
    "    a = np.array([g])\n",
    "    v = (1. - g * g) * r[0];\n",
    "    for i in range(1, p):\n",
    "        g = (r[i+1] - np.dot(a, r[1:i+1])) / v\n",
    "        a = np.r_[ g,  a - g * a[i-1::-1] ]\n",
    "        v *= 1. - g*g\n",
    "    # return the coefficients of the A(z) filter\n",
    "    return np.r_[1, -a[::-1]]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def bac(x, p):\n",
    "    # compute the biased autocorrelation for x up to lag p\n",
    "    L = len(x)\n",
    "    r = np.zeros(p+1)\n",
    "    for m in range(0, p+1):\n",
    "        for n in range(0, L-m):\n",
    "            r[m] += x[n] * x[n+m]\n",
    "        r[m] /= float(L)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lpc(x, p):\n",
    "    # compute p LPC coefficients for a speech segment\n",
    "    return ld(bac(x, p), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "let's plot the previous figure again by direct computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spec(vs, vsf)\n",
    "A = np.fft.fft(lpc(vs, 20), len(vs))\n",
    "plot_spec(np.abs(np.divide(1.0, A)), vsf, do_fft=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LPC-based pitch shifting\n",
    "\n",
    "Simple autotune algorithm based on granular synthesis\n",
    "\n",
    "For each input chunk:\n",
    " * compute the LPC coefficients for chunk\n",
    " * inverse-filter the chunk and recover the excitation signal\n",
    " * pitch-shift the excitation via resampling\n",
    " * forward-filter the shifted excitation to re-apply the original envelope.\n",
    " * combine the resulting grains using a tapered window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def pitchshift_gs_lpc_rt(x, alpha, grain_size, overlap=0.4, LPC_order=20):\n",
    "    win, stride = tapering_window(grain_size, overlap)\n",
    "    # size of input chunk before resampling\n",
    "    chunk_size = int(np.floor(grain_size + 1) * alpha)\n",
    "    filter_state = np.zeros(LPC_order)\n",
    "    y = np.zeros(len(x))\n",
    "    for n in range(0, len(x) - max(chunk_size, grain_size), stride):\n",
    "        chunk = x[n:n+chunk_size]\n",
    "        a = lpc(chunk, LPC_order)\n",
    "        exc = sp.lfilter(a, [1], chunk)\n",
    "        # this changes the length of exc from chunk_size to grain_size:\n",
    "        exc = resample(exc, 1 / alpha)\n",
    "        grain, filter_state = sp.lfilter([1], a, exc, zi=filter_state)\n",
    "        y[n:n+grain_size] += grain * win\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "alpha, grain_size = semitone ** 2, ms2n(100, ssf)\n",
    "multiplay([ys, pitchshift_gs_lpc_rt(ys, alpha, grain_size), pitchshift_gs_lpc_rt(ys, 1/alpha, grain_size)], ssf, title=['vocal sample', 'up two semitones', 'down two semitones'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding phase continuity\n",
    "\n",
    "The current implementation can be improved by ensuring phase continuity in the excitation. For this we need to\n",
    " * consider a longer chunk that yields two grains\n",
    " * compute the LPC coefficients only for the first half\n",
    " * extract the excitation for the whole chunk\n",
    " * compute the phase difference in the excitation to update global phase\n",
    " * resynthesize the excitation using the global phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def pitchshift_gs_lpc_pv_rt(x, alpha, grain_size, LPC_order=20):\n",
    "    win, stride, grain_size = hanning_window(grain_size)\n",
    "    # size of input chunk before resampling\n",
    "    chunk_size = int(np.floor(grain_size + 1) * alpha)\n",
    "    filter_state = np.zeros(LPC_order)\n",
    "    phase = np.zeros(grain_size)\n",
    "    y = np.zeros(len(x))\n",
    "    for n in range(0, len(x) - 2 * max(chunk_size, grain_size), stride):\n",
    "        a = lpc(x[n:n+chunk_size], LPC_order)\n",
    "        exc = sp.lfilter(a, [1], x[n:n+chunk_size+chunk_size])\n",
    "        exc = resample(exc, 1 / alpha)\n",
    "        exc_fft_curr = np.fft.fft(win * exc[0:grain_size])\n",
    "        exc_fft_next = np.fft.fft(win * exc[stride:stride+grain_size])\n",
    "        phase_diff = np.angle(exc_fft_next) - np.angle(exc_fft_curr)\n",
    "        grain_exc = np.real(np.fft.ifft(np.abs(exc_fft_curr) * np.exp(1j * phase)))\n",
    "        grain, filter_state = sp.lfilter([1], a, grain_exc, zi=filter_state)\n",
    "        y[n:n+grain_size] += grain * win\n",
    "        phase = phase + phase_diff  \n",
    "        phase = phase - 2 * np.pi * np.round(phase / (2 * np.pi))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "alpha, grain_size = semitone ** 2, ms2n(100, ssf)\n",
    "multiplay([ys, pitchshift_gs_lpc_rt(ys, alpha, grain_size), pitchshift_gs_lpc_pv_rt(ys, alpha, grain_size)], ssf, title=['vocal sample', 'up two semitones (GS-LPC)', 'up two semitones (GS-LPC-PV)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplay([ys, pitchshift_gs_lpc_rt(ys, 1 / alpha, grain_size), pitchshift_gs_lpc_pv_rt(ys, 1/alpha, grain_size)], ssf, title=['vocal sample', 'down two semitones (GS-LPC)', 'down two semitones (GS-LPC-PV)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A simple vocoder\n",
    "\n",
    "The LPC analysis can also be used to produce extremely artificial-sounding voices, as demonstrated here, where we replace the excitation siganl by a square wave of constant frequency. This is the type of sound created by the early [Vocoder](https://en.wikipedia.org/wiki/Vocoder) machines, for instance, and is still in use today to achieve some characteristic effects in popular music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def vocoder(x, pitch, sf, grain_size_ms=0, LPC_order=20):\n",
    "    grain_size = ms2n(grain_size_ms, sf) if grain_size_ms > 0 else ms2n(40, sf)\n",
    "    w, ph = 2 * np.pi * pitch / sf, 0\n",
    "    win, stride, grain_size = hanning_window(grain_size)\n",
    "    filter_state = np.zeros(LPC_order)\n",
    "    y = np.zeros(len(x))\n",
    "    for n in range(0, len(x) - grain_size, stride):\n",
    "        grain = x[n:n+grain_size]\n",
    "        a = lpc(grain, LPC_order)\n",
    "        if pitch < 0:\n",
    "            exc = np.random.rand(grain_size) - 0.5\n",
    "        elif pitch == 0:\n",
    "            exc = np.r_[1, np.zeros(grain_size - 1)]\n",
    "        else:\n",
    "            exc = np.sign(np.sin(ph + w * np.arange(0, grain_size)))\n",
    "            ph += w * stride\n",
    "        grain, filter_state = sp.lfilter([1], a, exc, zi=filter_state)\n",
    "        y[n:n+grain_size] += grain * win\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "multiplay([ss, vocoder(ss, -1, ssf), vocoder(ss, 140, ssf), vocoder(ss, 0, ssf)], ssf, title=['speech sample', 'whisper', 'daft', 'robot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplay([ys, vocoder(ys, -1, ssf), vocoder(ys, 140, ssf), vocoder(ys, 0, ssf)], ssf, title=['vocal sample', 'whisper', 'daft', 'robot'])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
